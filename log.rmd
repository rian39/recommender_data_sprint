```{r}
library(knitr)
library(rjson)
library(dplyr)
```


## Tue 12 Feb 2019 13:45:42 AEDT
- Youtube api for recommendations 
- https://developers.google.com/youtube/v3/docs/ is the starting point
- possible endpoints to use:
    - https://developers.google.com/youtube/v3/docs/videos for any id gives:


- would we need to consider Channels and possibly playlists? Channels are at https://developers.google.com/youtube/v3/docs/channels, in order to see what they picked as favourited?
- on my HomePage/channel, a lot of recommendations appear. Where did they come from?
- Subscriptions could be an endpoint: https://developers.google.com/youtube/v3/docs/subscriptions
- playlists might have recommendation: https://developers.google.com/youtube/v3/docs/playlists/list




## Thu 14 Feb 2019 11:16:02 AEDT
- Working with the reddit data
- Useful sentiment analysis at https://github.com/aleszu/reddit-sentiment-analysis; also has the python script for getting stuff


## Fri 15 Feb 2019 08:25:36 AEDT
- value of reddit users https://www.cnbc.com/2019/02/11/reddit-users-are-the-least-valuable-of-any-social-network.html
- list of top subreddits https://medium.com/@davis1/the-31-biggest-subreddits-f95c1f1f5e97

    1. funny — 19,895,474 subscribers

    2. AskReddit — 19,510,110

    3. todayilearned — 18,981,496

    4. worldnews — 18,977,231

    5. Science — 18,932,710

    6. pics — 18,886,430

    7. gaming — 18,413,642

    8. IAmA — 18,097,375

    9. videos — 17,973,194

    10. movies — 17,824,982

    11. aww — 17,433,245 pets

    12. Music — 17,051,188

    13. blog — 16,507,239

    14. gifs — 16,321,999

    15. news — 16,218,736

    16. explainlikeimfive — 15,705,452

    17. askscience — 15,634,059

    18. EarthPorn — 15,529,380

    19. books — 14,732,532

    20. television — 14,730,352

- found this paper on reddit https://www.reddit.com/r/science/comments/aqavuw/machine_learning_analysis_of_deleted_content_on/
- saved in ~/archive/ensembles/reddit_deletions_2019.pdf


## Mon 18 Feb 2019 14:26:19 AEDT
-  3 problems for the data sprint:
    1. karma reverse engineering
    2. dynamics of attention -- platforms vs internal; 
    3. moderation effects 


## Wed 20 Feb 2019 09:42:05 AEDT

```{r masterclass}
library(igraph)
library(vosonSML)
# check opensource software journal
library(quanteda)
library(readtext)
help.start()

cor  <- corpus('A corpus is a set of documents', 'A set of documents is text')
summary(cor)

tokens('Of all tax, income taxes are worst', remove_punct = TRUE)
tokens_wordstem(toks) %>% tokens_remove(stopwords('english')) %>% tolower()
tokens_wordstem(toks) %>% tokens_remove(stopwords('english')) %>% types()
tokens_wordstem(toks) %>% tokens_remove(stopwords('english')) %>% dfm()
df  <- tokens_wordstem(toks) %>% tokens_remove(stopwords('english')) %>% dfm()

# combine the tokenizing, etc, in the dfm
dfm(cor, remove_punct = TRUE, stem =  TRUE, tolower = TRUE, remove = stopwords('english'))

#corpus functions

corpus_subset()
corpus_reshape()
corpus_segment()
corpus_sample()

inc  <- quanteda::data_corpus_inaugural
summary(inc)

inc %>% corpus_subset(President == 'Obama') %>%
    corpus_reshape(to = 'sentences') %>% ndoc()

inc %>% corpus_subset(Year >= 2000) %>%
    corpus_reshape(to = 'sentences') %>% ndoc()

docvars(inc) %>% head()
docvars(inc, "Order")  <- 1:ndoc(inc)

# keywords in context
kwic(inc, pattern = 'security', window = 3) %>% head()
kwic(inc, pattern = 'security', window = 3) %>% textplot_xray()
kwic(inc, pattern = 'security', window = 3) %>% nrow()
k1  <- kwic(inc, pattern = phrase('God'), window = 3)
k2  <- kwic(inc, pattern = phrase('God Bless'), window = 3)
textplot_xray(k1,k2)

# select tmers
tok  <- tokens(inc)
tokens_select(tok, c('immig*', 'migra*'), padding = TRUE, window =5) %>% head()
tokens_select(tok, tokens_ngrams(n=2))

# dictionaries used to implement categories for words

dict  <-  dictionary(list(refugee = c('refugee', 'asylum'), worker =c('worker*','employe*')) )
data_dictionary_LSD2015
#other dictionaries on Benoit's repos?
# see https://github.com/kbenoit/quanteda.dictionaries

inc %>% tokens() %>% tokens_lookup(data_dictionary_LSD2015) %>% summary()
# measures

inc %>% dfm() %>% textstat_frequency()
docvars(inc)
inc %>% dfm() %>% dfm_group('President') %>% textstat_frequency()
textstat_dist(dfm(inc))

textstat_keyness(key_dfm )
textstat_collocations
textstat_lexdiv
textstat_readability

# textplot_network
toks <- corpus_subset(data_corpus_irishbudget2010) %>%
    tokens(remove_punct = TRUE) %>%
    tokens_tolower() %>%
    tokens_remove(stopwords("english"), padding = FALSE)
myfcm <- fcm(toks, context = "window", tri = FALSE)
feat <- names(topfeatures(myfcm, 30))
g  <- fcm_select(myfcm, feat, verbose = FALSE) %>% 
    textplot_network(min_freq = 0.5)

library(ggplot2)
g + ggtitle('test title')
g
```

## Gephi session

J Drucker article on graphical display 

## the sprint

[Infrastructures of karma: an inquiry into attention metrification and attention dynamics in reddit] ( https://drive.google.com/open?id=1krqUNu2o1rYA-q_234860F-0Dd4k1gpjT3A_PcEG390)

How and why do some conversations go down different paths? Reddit is a between space

subprojects:
    1. Instant Karma 
    2. Karma Chameleon
    3. Karma Police
Matias, J. Nathan. 2016. “Going Dark: Social Factors in Collective Action Against Platform Operators in the Reddit Blackout.” In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. ACM. 


