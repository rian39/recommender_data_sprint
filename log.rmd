```{r}
library(knitr)
library(rjson)
library(dplyr)
```


## Tue 12 Feb 2019 13:45:42 AEDT
- Youtube api for recommendations 
- https://developers.google.com/youtube/v3/docs/ is the starting point
- possible endpoints to use:
    - https://developers.google.com/youtube/v3/docs/videos for any id gives:


- would we need to consider Channels and possibly playlists? Channels are at https://developers.google.com/youtube/v3/docs/channels, in order to see what they picked as favourited?
- on my HomePage/channel, a lot of recommendations appear. Where did they come from?
- Subscriptions could be an endpoint: https://developers.google.com/youtube/v3/docs/subscriptions
- playlists might have recommendation: https://developers.google.com/youtube/v3/docs/playlists/list




## Thu 14 Feb 2019 11:16:02 AEDT
- Working with the reddit data
- Useful sentiment analysis at https://github.com/aleszu/reddit-sentiment-analysis; also has the python script for getting stuff


## Fri 15 Feb 2019 08:25:36 AEDT
- value of reddit users https://www.cnbc.com/2019/02/11/reddit-users-are-the-least-valuable-of-any-social-network.html
- list of top subreddits https://medium.com/@davis1/the-31-biggest-subreddits-f95c1f1f5e97

    1. funny — 19,895,474 subscribers

    2. AskReddit — 19,510,110

    3. todayilearned — 18,981,496

    4. worldnews — 18,977,231

    5. Science — 18,932,710

    6. pics — 18,886,430

    7. gaming — 18,413,642

    8. IAmA — 18,097,375

    9. videos — 17,973,194

    10. movies — 17,824,982

    11. aww — 17,433,245 pets

    12. Music — 17,051,188

    13. blog — 16,507,239

    14. gifs — 16,321,999

    15. news — 16,218,736

    16. explainlikeimfive — 15,705,452

    17. askscience — 15,634,059

    18. EarthPorn — 15,529,380

    19. books — 14,732,532

    20. television — 14,730,352

- found this paper on reddit https://www.reddit.com/r/science/comments/aqavuw/machine_learning_analysis_of_deleted_content_on/
- saved in ~/archive/ensembles/reddit_deletions_2019.pdf


## Mon 18 Feb 2019 14:26:19 AEDT
-  3 problems for the data sprint:
    1. karma reverse engineering
    2. dynamics of attention -- platforms vs internal; 
    3. moderation effects 


## Wed 20 Feb 2019 09:42:05 AEDT

```{r masterclass}
library(igraph)
library(vosonSML)
# check opensource software journal
library(quanteda)
library(readtext)
help.start()

cor  <- corpus('A corpus is a set of documents', 'A set of documents is text')
summary(cor)

tokens('Of all tax, income taxes are worst', remove_punct = TRUE)
tokens_wordstem(toks) %>% tokens_remove(stopwords('english')) %>% tolower()
tokens_wordstem(toks) %>% tokens_remove(stopwords('english')) %>% types()
tokens_wordstem(toks) %>% tokens_remove(stopwords('english')) %>% dfm()
df  <- tokens_wordstem(toks) %>% tokens_remove(stopwords('english')) %>% dfm()

# combine the tokenizing, etc, in the dfm
dfm(cor, remove_punct = TRUE, stem =  TRUE, tolower = TRUE, remove = stopwords('english'))

#corpus functions

corpus_subset()
corpus_reshape()
corpus_segment()
corpus_sample()

inc  <- quanteda::data_corpus_inaugural
summary(inc)

inc %>% corpus_subset(President == 'Obama') %>%
    corpus_reshape(to = 'sentences') %>% ndoc()

inc %>% corpus_subset(Year >= 2000) %>%
    corpus_reshape(to = 'sentences') %>% ndoc()

docvars(inc) %>% head()
docvars(inc, "Order")  <- 1:ndoc(inc)

# keywords in context
kwic(inc, pattern = 'security', window = 3) %>% head()
kwic(inc, pattern = 'security', window = 3) %>% textplot_xray()
kwic(inc, pattern = 'security', window = 3) %>% nrow()
k1  <- kwic(inc, pattern = phrase('God'), window = 3)
k2  <- kwic(inc, pattern = phrase('God Bless'), window = 3)
textplot_xray(k1,k2)

# select tmers
tok  <- tokens(inc)
tokens_select(tok, c('immig*', 'migra*'), padding = TRUE, window =5) %>% head()
tokens_select(tok, tokens_ngrams(n=2))

# dictionaries used to implement categories for words

dict  <-  dictionary(list(refugee = c('refugee', 'asylum'), worker =c('worker*','employe*')) )
data_dictionary_LSD2015
#other dictionaries on Benoit's repos?
# see https://github.com/kbenoit/quanteda.dictionaries

inc %>% tokens() %>% tokens_lookup(data_dictionary_LSD2015) %>% summary()
# measures

inc %>% dfm() %>% textstat_frequency()
docvars(inc)
inc %>% dfm() %>% dfm_group('President') %>% textstat_frequency()
textstat_dist(dfm(inc))

textstat_keyness(key_dfm )
textstat_collocations
textstat_lexdiv
textstat_readability

# textplot_network
toks <- corpus_subset(data_corpus_irishbudget2010) %>%
    tokens(remove_punct = TRUE) %>%
    tokens_tolower() %>%
    tokens_remove(stopwords("english"), padding = FALSE)
myfcm <- fcm(toks, context = "window", tri = FALSE)
feat <- names(topfeatures(myfcm, 30))
g  <- fcm_select(myfcm, feat, verbose = FALSE) %>% 
    textplot_network(min_freq = 0.5)

library(ggplot2)
g + ggtitle('test title')
g
```

## Gephi session

J Drucker article on graphical display 

## the sprint

[Infrastructures of karma: an inquiry into attention metrification and attention dynamics in reddit] ( https://drive.google.com/open?id=1krqUNu2o1rYA-q_234860F-0Dd4k1gpjT3A_PcEG390)

How and why do some conversations go down different paths? Reddit is a between space

subprojects:
    1. Instant Karma 
    2. Karma Chameleon
    3. Karma Police
Matias, J. Nathan. 2016. “Going Dark: Social Factors in Collective Action Against Platform Operators in the Reddit Blackout.” In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. ACM. 


## Mon 25 Mar 2019 13:52:26 AEDT
- talking about possible paper:



Dear Rob and Adrian,

Here as promised a few ideas on how we can bring forward the work that we initiated during our data sprint. My proposal, unsurprisingly, is inspired by my current research on attention hyper-synchronisation. Yet this is only pitch and open to take other research directions if you have different ideas.

As you know, my main curiosity lies at present in the acceleration and amplification of online attention cycles. In this regard, there are three things that I’d like to explore in Reddit:

    Is it possible to identify such acceleration and amplification phenomena in Reddit?
    Given a subreddit, can we define the ‘normal waveform’ of the attention cycle of its threads (attention can be operationalised as number of votes or comments or a combination of both)? Can we identify hyper-synchronized threads (or phases within threads) whose waveform have significantly shorter frequency and larger amplitude?

    Is there a correlation between the hyper-synchronisation of attention and the degradation of debate quality?
    Can we find indicators of debate quality (e.g. number of words in posts, diversity of the language used) and observe how they vary in relation to normal/hyper-synchronized attention cycles?

    Can we detect the triggers of hyper-synchronisation?
    Can we identify the posts that triggers phases of hyper-synchronisation and describe their features (e.g. use of trolling language, high level of controversiality according to reddit measure…)?

I think that Reddit is an ideal place to explore these ideas (because its being somewhat in-between 4chan-like and stackoverflow-like forums).
“Menosphere” subreddits may offer a particularly interesting case study, because:

    they are highly relevant both sociologically and societally;
    they would allow to compare quarantined VS non-quarantined subreddits;
    (last but not least) we could rely on Simon’s qualitative exploration of the same phenomena.

If you are up for it, we could meet (possibly with Simons and others participants from the Sprint) tomorrow or in the first days of next week to discuss all this.

Let me know
Tommaso

-- 

Tommaso Venturini |  tommasoventurini.it
Center for Internet and Society
CNRS (National Center for Scientific Research, France)

- so ideas on this:
    - plot the waveforms for subreddits
    - look for spikes; 
    - look for trolls; troll detectors are being done;
    - look for correlation between quality of debate and synchronisation;  
- pushshift.io has graphs
- ideas for funding for grad student to do some of the data work; Simon? Galen? DH person? 

## Tue 26 Mar 2019 12:18:05 AEDT
- much of the realtime data is in pushshift.rt_reddit.comments and pushshift.rt_reddit.submissions on gbq.  This seems to realtime.

```
SELECT subreddit, count(*) FROM `pushshift.rt_reddit.comments` GROUP BY 1 ORDER BY 2 DESC limit 100
```

- also on Github, there is another API https://github.com/pushshift/api
