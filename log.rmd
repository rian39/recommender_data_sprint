```{r}
library(knitr)
library(rjson)
library(dplyr)
```


## Tue 12 Feb 2019 13:45:42 AEDT
- Youtube api for recommendations 
- https://developers.google.com/youtube/v3/docs/ is the starting point
- possible endpoints to use:
    - https://developers.google.com/youtube/v3/docs/videos for any id gives:


- would we need to consider Channels and possibly playlists? Channels are at https://developers.google.com/youtube/v3/docs/channels, in order to see what they picked as favourited?
- on my HomePage/channel, a lot of recommendations appear. Where did they come from?
- Subscriptions could be an endpoint: https://developers.google.com/youtube/v3/docs/subscriptions
- playlists might have recommendation: https://developers.google.com/youtube/v3/docs/playlists/list




## Thu 14 Feb 2019 11:16:02 AEDT
- Working with the reddit data
- Useful sentiment analysis at https://github.com/aleszu/reddit-sentiment-analysis; also has the python script for getting stuff


## Fri 15 Feb 2019 08:25:36 AEDT
- value of reddit users https://www.cnbc.com/2019/02/11/reddit-users-are-the-least-valuable-of-any-social-network.html
- list of top subreddits https://medium.com/@davis1/the-31-biggest-subreddits-f95c1f1f5e97

    1. funny — 19,895,474 subscribers

    2. AskReddit — 19,510,110

    3. todayilearned — 18,981,496

    4. worldnews — 18,977,231

    5. Science — 18,932,710

    6. pics — 18,886,430

    7. gaming — 18,413,642

    8. IAmA — 18,097,375

    9. videos — 17,973,194

    10. movies — 17,824,982

    11. aww — 17,433,245 pets

    12. Music — 17,051,188

    13. blog — 16,507,239

    14. gifs — 16,321,999

    15. news — 16,218,736

    16. explainlikeimfive — 15,705,452

    17. askscience — 15,634,059

    18. EarthPorn — 15,529,380

    19. books — 14,732,532

    20. television — 14,730,352

- found this paper on reddit https://www.reddit.com/r/science/comments/aqavuw/machine_learning_analysis_of_deleted_content_on/
- saved in ~/archive/ensembles/reddit_deletions_2019.pdf


## Mon 18 Feb 2019 14:26:19 AEDT
-  3 problems for the data sprint:
    1. karma reverse engineering
    2. dynamics of attention -- platforms vs internal; 
    3. moderation effects 


## Wed 20 Feb 2019 09:42:05 AEDT

```{r masterclass}
library(igraph)
library(vosonSML)
# check opensource software journal
library(quanteda)
library(readtext)
help.start()

cor  <- corpus('A corpus is a set of documents', 'A set of documents is text')
summary(cor)

tokens('Of all tax, income taxes are worst', remove_punct = TRUE)
tokens_wordstem(toks) %>% tokens_remove(stopwords('english')) %>% tolower()
tokens_wordstem(toks) %>% tokens_remove(stopwords('english')) %>% types()
tokens_wordstem(toks) %>% tokens_remove(stopwords('english')) %>% dfm()
df  <- tokens_wordstem(toks) %>% tokens_remove(stopwords('english')) %>% dfm()

# combine the tokenizing, etc, in the dfm
dfm(cor, remove_punct = TRUE, stem =  TRUE, tolower = TRUE, remove = stopwords('english'))

#corpus functions

corpus_subset()
corpus_reshape()
corpus_segment()
corpus_sample()

inc  <- quanteda::data_corpus_inaugural
summary(inc)

inc %>% corpus_subset(President == 'Obama') %>%
    corpus_reshape(to = 'sentences') %>% ndoc()

inc %>% corpus_subset(Year >= 2000) %>%
    corpus_reshape(to = 'sentences') %>% ndoc()

docvars(inc) %>% head()
docvars(inc, "Order")  <- 1:ndoc(inc)

# keywords in context
kwic(inc, pattern = 'security', window = 3) %>% head()
kwic(inc, pattern = 'security', window = 3) %>% textplot_xray()
kwic(inc, pattern = 'security', window = 3) %>% nrow()
k1  <- kwic(inc, pattern = phrase('God'), window = 3)
k2  <- kwic(inc, pattern = phrase('God Bless'), window = 3)
textplot_xray(k1,k2)

# select tmers
tok  <- tokens(inc)
tokens_select(tok, c('immig*', 'migra*'), padding = TRUE, window =5) %>% head()
tokens_select(tok, tokens_ngrams(n=2))

# dictionaries used to implement categories for words

dict  <-  dictionary(list(refugee = c('refugee', 'asylum'), worker =c('worker*','employe*')) )
data_dictionary_LSD2015
#other dictionaries on Benoit's repos?
# see https://github.com/kbenoit/quanteda.dictionaries

inc %>% tokens() %>% tokens_lookup(data_dictionary_LSD2015) %>% summary()
# measures

inc %>% dfm() %>% textstat_frequency()
docvars(inc)
inc %>% dfm() %>% dfm_group('President') %>% textstat_frequency()
textstat_dist(dfm(inc))

textstat_keyness(key_dfm )
textstat_collocations
textstat_lexdiv
textstat_readability

# textplot_network
toks <- corpus_subset(data_corpus_irishbudget2010) %>%
    tokens(remove_punct = TRUE) %>%
    tokens_tolower() %>%
    tokens_remove(stopwords("english"), padding = FALSE)
myfcm <- fcm(toks, context = "window", tri = FALSE)
feat <- names(topfeatures(myfcm, 30))
g  <- fcm_select(myfcm, feat, verbose = FALSE) %>% 
    textplot_network(min_freq = 0.5)

library(ggplot2)
g + ggtitle('test title')
g
```

## Gephi session

J Drucker article on graphical display 

## the sprint

[Infrastructures of karma: an inquiry into attention metrification and attention dynamics in reddit] ( https://drive.google.com/open?id=1krqUNu2o1rYA-q_234860F-0Dd4k1gpjT3A_PcEG390)

How and why do some conversations go down different paths? Reddit is a between space

subprojects:
    1. Instant Karma 
    2. Karma Chameleon
    3. Karma Police
Matias, J. Nathan. 2016. “Going Dark: Social Factors in Collective Action Against Platform Operators in the Reddit Blackout.” In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. ACM. 


## Mon 25 Mar 2019 13:52:26 AEDT
- talking about possible paper:



Dear Rob and Adrian,

Here as promised a few ideas on how we can bring forward the work that we initiated during our data sprint. My proposal, unsurprisingly, is inspired by my current research on attention hyper-synchronisation. Yet this is only pitch and open to take other research directions if you have different ideas.

As you know, my main curiosity lies at present in the acceleration and amplification of online attention cycles. In this regard, there are three things that I’d like to explore in Reddit:

    Is it possible to identify such acceleration and amplification phenomena in Reddit?
    Given a subreddit, can we define the ‘normal waveform’ of the attention cycle of its threads (attention can be operationalised as number of votes or comments or a combination of both)? Can we identify hyper-synchronized threads (or phases within threads) whose waveform have significantly shorter frequency and larger amplitude?

    Is there a correlation between the hyper-synchronisation of attention and the degradation of debate quality?
    Can we find indicators of debate quality (e.g. number of words in posts, diversity of the language used) and observe how they vary in relation to normal/hyper-synchronized attention cycles?

    Can we detect the triggers of hyper-synchronisation?
    Can we identify the posts that triggers phases of hyper-synchronisation and describe their features (e.g. use of trolling language, high level of controversiality according to reddit measure…)?

I think that Reddit is an ideal place to explore these ideas (because its being somewhat in-between 4chan-like and stackoverflow-like forums).
“Menosphere” subreddits may offer a particularly interesting case study, because:

    they are highly relevant both sociologically and societally;
    they would allow to compare quarantined VS non-quarantined subreddits;
    (last but not least) we could rely on Simon’s qualitative exploration of the same phenomena.

If you are up for it, we could meet (possibly with Simons and others participants from the Sprint) tomorrow or in the first days of next week to discuss all this.

Let me know
Tommaso

-- 

Tommaso Venturini |  tommasoventurini.it
Center for Internet and Society
CNRS (National Center for Scientific Research, France)

- so ideas on this:
    - plot the waveforms for subreddits
    - look for spikes; 
    - look for trolls; troll detectors are being done;
    - look for correlation between quality of debate and synchronisation;  
- pushshift.io has graphs
- ideas for funding for grad student to do some of the data work; Simon? Galen? DH person? 

## Tue 26 Mar 2019 12:18:05 AEDT
- much of the realtime data is in pushshift.rt_reddit.comments and pushshift.rt_reddit.submissions on gbq.  This seems to realtime.

```
SELECT subreddit, count(*) FROM `pushshift.rt_reddit.comments` GROUP BY 1 ORDER BY 2 DESC limit 100
```

- also on Github, there is another API https://github.com/pushshift/api

## Fri 05 Apr 2019 12:41:05 AEDT
- application for ANU-CNRS coop has gone in; see emails and RAPID_CNRS-ANU.pdf
- writing small bid to RSSS for funds.

Dear Professor Waldby

## Platform news and hyper-synchronised attention dynamics 

We (Ackland and Mackenzie) recently submitted a application with Dr Tommaso Venturini (RSSS Visiting Research Fellow, March 2019; CNRS Paris) to the ANU-CNRS (Paris) Collaboration Program to fund PhD studentships in the area of policy analytics and social media platforms. We have been working specifically on how platforms accelerate attention cycles.  This work arose out of collaborative work started during Dr Venturini's time at the School of Sociology, and in particular the 'Data Sprint' we ran in March. We seek funding to assist in timely completion of analysis started in the data sprint. 

We would like funding to employ a research assistant to gather more data and, under our supervision, assist in the development of  analysis of 'attentional synchronisation' on social media platforms.  The work will lead to a publication output in a journal such as _Media, Culture and Society_. It will also help continue a collaboration of intellectual and institutional substance focused on platform-information dynamics and attentional economies. Carrying out this work is also immediately relevant to current debates over social media platform regulation. It will  continue to build computational social science capacity in CASS, an area of development to which we are both actively pursuing. 

## Costing

80 hours (May-July 2019) @58.06/hr for RA ANU Level 5  \$5224.50

## Mon 08 Apr 2019 15:52:56 AEST
- sent this proposal to Waldby

## Mon 06 May 2019 12:02:08 AEST
- meeting with Galen, Rob & Simon
- attention vs engagement; and how to do that; api;     
- possible subreddits /r/politics; front page; 
- start with  Kotaku and Futurology; 
- meeting next week to catch up on initial data work; Galen starting;  

## Mon 13 May 2019 11:37:07 AEST
- meeting with Galen and Simon on first scripts on scraping; 
    - problems - startup time is a week; can only do one thread at a time; so the timestamps will vary; 
- gitlab  repos -- https://gitlab.anu.edu.au/u6312704/reddit-scrape  
- use top, hot, controversial, new - what time period for top? 
- do all threads only last a few days -- how would we verify that? can use pushshift to verify this? 
- differences in volume on subbreddits - does that 
    - votes vs comments


## Tue 28 May 2019 09:05:18 AEST

## Galens progress report for 1st scrape

A quick update on the reddit scraping trial:

Current Code & Trial is Very Rough, But Is Now (Sort of) Working
I currently have my two office computers running 'scrape.py' --- one for /r/KotakuInAction and one for /r/Futurology. These have been moderately successful, although there are substantial gaps in execution (see issues listed below). I've attached two CSVs that provide a basic report on the status of each thread in the subreddits. Basically, most threads have been frequently snapshotted by the metadata scrape and barely ever covered by the full thread (with comments) scrape. The /r/KotakuInAction scrape has observed (and is attempting to repeatedly scrape) 586 threads; the /r/Futurology is covering 997. As of yesterday afternoon, data from both is uploaded as-it's-scraped to a Dropbox folder (available [here](tk)).

The number of 'active' observed threads has been increasing, but it will level off eventually (as both scrapers settle on some steady-ish state of 'number of threads that are usually active in a given week in that subreddit'). I have no idea what that steady state number will be (but it matters a lot, see below).


Current Problems:

Problem 1: Errors That Occur Without 'Unlocking' Cause The Scheduled Scrape To Stop
To prevent a race condition in which multiple instances of 'scrape.py' are run concurrently and end up repeatedly scraping the same thread (as a result of, say, a cron job that starts a run of 'scrape.py' every minute, when a single post ends up taking more than a minute to scrape), 'scrape.py' currently checks for the existence of a file called '.queue.lock' in the data folder and only scrapes if the file is not found. This is great (if rough) in theory, but sometimes a scrape will fail/hang without deleting the .queue.lock file. I only discovered this recently.

Possible Solution: Forget About Locking
My temporary solution has been to delete the .queue.lock file if it's more than 45 minutes since the most recent scrape file was added. This isn't great.


Problem 2: Full Thread Snapshots Are Incredibly Infrequent
Because of the number of threads being monitored, and the time it takes to scrape them, the average gap between snapshots of a given thread's comments is currently *huge*. 'scrape.py' guarantees execution of a metadata scrape of the subreddit every half hour, but otherwise chooses whatever 'active' thread that we haven't scraped in the longest time. /r/Futurology currently has 912 active-and-monitored threads, and /r/KotakuInAction has 581. The effect of this is that, even if a full thread-with-comments scrape occured every minute, we would only snapshot a given /r/Futurology thread every ~15 hours.

Possible Solution: Decoupling Metadata and Full Thread Scraping
By scheduling a metadata scrape every (say) 15 minutes on one computer, and then running a large number of thread scrapes in parallel on separate computers (each with a separate API client id/secret/useragent), we may be able to push the snapshot gaps down significantly. Unfortunately, this will require that the scraping system is containerised is some way, ideally so that it can be run on AWS instances (or whatever). I've been working on this, and have one AWS instance contributing to the futurology scrape at the moment.


Problem 3: Data Storage
The current scrape data takes up around 3Gb. Most of this is due (interestingly?) to a handful of large-and-repeatedly-snapshotted comment threads.

Solution: Periodically Archive Old (ie, 3 week old?) Snapshots
This is mostly a scheduling issue, and I'm looking into it.


I'm trying not to break the current functionality of the scripts in the gitlab repo, but I'd like to do a few merges soon and clean stuff up. Please feel free to file issues via the GitLab (or just email me). Let me know what's needed.

I'm working on a script to convert the directories of raw JSON files into an R dataframe (of comments) at the moment. This will be crunchy.

Cheers,
Galen


## Thu 30 May 2019 12:14:43 AEST
- talking about the above problems; 
- tactics -- limit the activity filter; distribute the scrape, use more computers, prioritise the threads; 
- build graphs to get a handle on this; 
- what do we do with the data? Discussion on this; metadata should be fine; 

## Sun 02 Jun 2019 13:50:34 AEST
- lots of work on metadata this weekend
- todo: add cloudstor as means of sharing files 
- read about hclust and distance metrics -- the one I need has to apply to counts and not worry too much about differences in scale ...

## Tue 11 Jun 2019 09:58:50 AEST
- running scraper a lot to test it on humanbeingbros; using nectar server to do this; 
- found a couple of articles on reddit in Social Media + Society; in zotero ensembles; quite a lot on engagement, etc.
- the journal - social media + society

## Fri 14 Jun 2019 11:33:34 AEST
- the scrape is still running; but a bit too slow; metadata takes as long as the scrape itself;
https://www.dropbox.com/sh/jz1pmg8tkegdar1/AADwBCoRpZK0Akeuicu7NQz-a?dl=0
- limit the creation date to threads; 
- going back to tv's late march email shown above;  
- hawke's point process; econometric packages; changepoint packages;   

## Fri 26 Jul 2019 11:05:42 AEST

- lambda aws \$1000 bill
- need to be added to the cloudstor share for the processed jsons;  
- rob has been building graphs; will post into a new branch;  what is he trying to do with this? Gonzaler, Baylon -- tree structures -- dynamics of posts; plots for breadth vs depth;   this is a proxy for engagement;
- need for upvotes and downvotes for attention;  Simon, will start on this using my code; 
- could use the comments field  to get into the forms of attention and how affect relates to this?  
- threadids have some interesting info in them;  

## Tue 06 Aug 2019 16:06:55 AEST
-  can bid for some funding with https://services.anu.edu.au/research-support/funding-opportunities/anu-global-research-partnerships-scheme; 
- TODO: put the document template in google doc and share; 
